Namespace(batch_size=64, epoch=25, load_file='/home/victor/dataset/img_align_celeba', lrD=0.0004, lrG=0.0001, ndf=32, ngf=32, param=0.9, save_file='./log/base')
CNetG(
  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (convT1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (convT2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (convT3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (convT4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (att): Self_AttentionLayer(
    (f_layer): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
    (g_layer): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
    (h_layer): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    (sm_layer): Softmax()
  )
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.2)
)
CNetD(
  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
  (att): Self_AttentionLayer(
    (f_layer): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
    (g_layer): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
    (h_layer): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (sm_layer): Softmax()
  )
  (leaky_relu): LeakyReLU(negative_slope=0.2)
)
